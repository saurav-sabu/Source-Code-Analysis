{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir code_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'c:\\\\GenAI\\\\GenAI Projects\\\\Source-Code-Analysis\\\\experiments\\\\code_base\\\\.git'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"code_base/\"\n",
    "\n",
    "Repo.clone_from(\"https://github.com/saurav-sabu/Question_Answering_System_Using_Gemini\",to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path+\"/src\",\n",
    "                                       glob=\"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser=LanguageParser(language=Language.PYTHON,parser_threshold=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'code_base\\\\src\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='from llama_index.core import SimpleDirectoryReader\\nfrom exception import CustomException\\nfrom logger import logging\\n\\nimport sys\\n\\ndef load_data(data):\\n\\n    try:\\n        logging.info(\"Data loading started------\")\\n        loader = SimpleDirectoryReader(\"data\")\\n        documents = loader.load_data()\\n\\n        logging.info(\"Data loading completed-----\")\\n        return documents\\n    \\n    except Exception as e:\\n        logging.info(\"Exception in loading data-------\")\\n        raise CustomException(e,sys)\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'code_base\\\\src\\\\embedding.py', 'language': <Language.PYTHON: 'python'>}, page_content='from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext, load_index_from_storage\\nfrom llama_index.embeddings.gemini import GeminiEmbedding\\n\\nfrom src.data_ingestion import load_data\\nfrom src.model_api import load_model\\n\\nimport sys\\nfrom exception import CustomException\\nfrom logger import logging \\n\\n\\ndef download_gemini_embedding(model,document):\\n\\n    try:\\n        logging.info(\"Embedding started to download -------\")\\n        gemini_embedding_model = GeminiEmbedding(model_name=\"models/embedding-001\")\\n        service_context = ServiceContext.from_defaults(llm=model,embed_model=gemini_embedding_model,chunk_size=800,chunk_overlap=20)\\n\\n        logging.info(\"Storing Vector embedding--------\")\\n        index = VectorStoreIndex.from_documents(document,service_context=service_context)\\n        index.storage_context.persist()\\n\\n\\n        logging.info(\"Querying results------\")\\n        query_engine = index.as_query_engine()\\n        return query_engine\\n    \\n    except Exception as e:\\n        raise CustomException(e,sys)\\n\\n\\n'),\n",
       " Document(metadata={'source': 'code_base\\\\src\\\\model_api.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom dotenv import load_dotenv\\nimport sys\\n\\nfrom llama_index.llms.gemini import Gemini\\nfrom exception import CustomException\\nfrom logger import logging\\n\\nimport google.generativeai as genai\\n\\nload_dotenv()\\n\\ngoogle_api_key = os.getenv(\"GOOGLE_API_KEY\")\\n\\ngenai.configure(api_key=google_api_key)\\n\\ndef load_model():\\n\\n    try:\\n        model = Gemini(models=\"gemini-pro\",api_key=google_api_key)\\n        return model\\n    except Exception as e:\\n        raise CustomException(e,sys)'),\n",
       " Document(metadata={'source': 'code_base\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON,\n",
    "                                                        chunk_size=2000,\n",
    "                                                        chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts,embedding=embedding,persist_directory=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationalRetrievalChain.from_llm(llm,\n",
    "                                              retriever=vectordb.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":3}),\n",
    "                                              memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The `download_gemini_embedding` function is responsible for downloading a Gemini embedding model, storing the vector embedding, and querying the results. It uses a GeminiEmbedding model with the specified model name \"models/embedding-001\" to create an embedding model. The function then creates a ServiceContext with default values for parameters like chunk size and overlap, and uses this context to create a VectorStoreIndex from the provided document. Finally, it persists the storage context and returns a query engine for querying the results. If an exception occurs during this process, a CustomException is raised.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"tell me about download_gemini_embedding function?\"\n",
    "\n",
    "result = chain.invoke(question)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
